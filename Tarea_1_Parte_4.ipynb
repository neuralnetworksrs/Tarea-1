{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea 1 Parte 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralnetworksrs/Tarea-1/blob/master/Tarea_1_Parte_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Y1tHj3DNqhX"
      },
      "source": [
        "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
        "\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> INF-395 Redes Neuronales Artificiales I-2018 </h1>\n",
        "\n",
        "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "**Temas**  \n",
        "\n",
        "* Entrenamiento de redes *Feed-Forward* vı́a GD y variantes (SGD, mini-*batches*), *momentum*, regularización y tasa de aprendizaje adaptiva.\n",
        "* Rol de capas ocultas y mayor profundidad (*Deep Learning*).\n",
        "* Diseño y entrenamiento de Redes Neuronales Convolucionales (CNNs).\n",
        "* Aplicaciones de las Redes Neuronales Convolucionales\n",
        "* Técnicas de regularización: *Dropout* y *Batch Normalization* \n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "La tarea se divide en secciones:\n",
        "\n",
        "1. Red Neuronal *Feed Forward* para Detectar Exoplanetas  \n",
        "2. *Deep Networks*  \n",
        "3. Redes Convolucionales en Imágenes  \n",
        "4. CNN *vs* RNN Prediciendo el Ozono Atmosférico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqy-qzkqwKiy",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"cuarto\"></a>\n",
        "## 4. CNN *vs* RNN Prediciendo el Ozono Atmosférico\n",
        "---\n",
        "\n",
        "En esta actividad utilizaremos redes neuronales para predicción de series de tiempo. Dada una serie de registros del valor de una variable en el tiempo (serie de tiempo) nos interesa entrenar un modelo que pueda predecir los valores futuros de la misma serie. \n",
        "\n",
        "En este caso, la serie corresponde a las concentraciones diarias (máximas) de $O_3$ medidas en la comuna de Las Condes entre Diciembre de 2003 y Diciembre de 2016. El $O_3$ (a nivel respirable) es un contaminante con graves efectos sobre la salud humana y sobre la agricultura, por lo que monitorear y predecir sus concentraciones en el tiempo es de gran relevancia. Se sabe que el $0_3$ alcanza sus niveles máximos en verano, por lo que estudiaremos sólo esos períodos. Esto hace que la serie que usted recibirá no es totalmente continua en el tiempo en el sentido de que los registros se miden hasta marzo de un año y se reinician en noviembre de ese mismo año. **Por favor considere atentamente esta situación.**   \n",
        "\n",
        "El archivo *CSV* que usted recibirá tiene las siguientes columnas \n",
        "\n",
        "[‘registered_on’, ‘CO', 'PM10', 'PM25', 'NO', 'NOX', 'WD', 'RH', 'TEMP', 'WS', 'UVA', 'UVB', 'O3']\n",
        "\n",
        "El primero corresponde a la fecha del registro. La última columna corresponde a la variable que queremos predecir y las demás son mediciones de otros contaminantes y de variables meteorológicas que podrían ayudar a predecir los niveles de $O_3$.  \n",
        "\n",
        "Su tarea consistirá en entrenar una red capaz de predecir los niveles de ozono el día de mañana sólo partir de los niveles *previas* de ozono y demás variables. Para ésto pre-procesaremos los datos, cada contaminante tendrá su serie de tiempo y asociada con las otras. Con ésto deberemos generar *secuencias de entrenamiento*, recordar que cada secuencia estará asocaida a un *Target* que es el $O_3$ del día siguiente.\n",
        "\n",
        "Compararemos y aplicaremos redes convolucionales uni-dimensionales y redes neuronales recurrentes, las cuales se adaptan perfectamente a escenarios de secuencias ya que modelan que el estado actual se genera a partir del estado anterior (como en una cadena de Markov).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNDHpbGFwKi0",
        "colab_type": "text"
      },
      "source": [
        "**a)** Explore los datos y visualícelos, ya sea a través de medidas de tendencia o gráficos. Lo primero que deberá realizar será crear su conjunto de validación/test a partir de los datos, por ejemplo las últimas mediciones puesto que la tarea será predecir los datos futuros (*out of box*). Además escale los datos apropiadamente.\n",
        "#### Solo con input: Ozono (1-D)\n",
        "Considere para construir las secuencias de entrenamiento sólo la información previa del $O_3$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJNBixkLcxIl",
        "colab_type": "code",
        "outputId": "c2697f8d-56a6-488a-8501-2f15fa504dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-1HVvUPwKi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qru5fE50kx5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def values(elements):\n",
        "  results = []\n",
        "  for index, val in enumerate(elements):\n",
        "    if int(val.split('-')[1]) < 5:\n",
        "      results.append(val)\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_4-HP_pcEzH",
        "colab_type": "code",
        "outputId": "96323cf9-35bd-49b8-d74c-c23ad71521c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "import pandas  as pd\n",
        "import numpy as np\n",
        "df_sets = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/Redes Neuronales/data/ozone_data.csv\")\n",
        "print(values(df_sets['registered_on']))\n",
        "mask_train = (df_sets['registered_on'][0:800]).values\n",
        "mask_test = (df_sets['registered_on'][0:800]).values\n",
        "df_labels_train = df_sets[values(df_sets['registered_on'])]\n",
        "df_labels_test = df_sets[0:800]\n",
        "df_X_train = df_sets[0:800]\n",
        "\n",
        "df_X_test = df_sets[0:800]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-46b3e08c2640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmask_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'registered_on'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmask_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'registered_on'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'registered_on'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_labels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_X_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([  18,   19,   20,   21,   22,   23,   24,   25,   26,   27,\\n            ...\\n            1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602],\\n           dtype='int64', length=956)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aO_hlDGcMX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = (df_labels_train[\"O3\"]).values\n",
        "y_test = (df_labels_test[\"O3\"]).values\n",
        "df_X_train = df_X_train.reset_index(drop=True)\n",
        "df_X_test = df_X_test.reset_index(drop=True)\n",
        "df_X_train.fillna(df_X_train.median(), inplace=True)\n",
        "df_X_test.fillna(df_X_test.median(), inplace=True)\n",
        "X_train = df_X_train.values[:,1:]\n",
        "X_test = df_X_test.values[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijAD-XNxf6fr",
        "colab_type": "code",
        "outputId": "8fc8cd8f-b4f8-4b19-ffe5-0503c8e9160e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "columns_names = df_X_train.columns[1:]\n",
        "print(columns_names)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "df_X_train.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['CO', 'PM10', 'PM25', 'NO', 'NOX', 'WD', 'RH', 'TEMP', 'WS', 'UVA',\n",
            "       'UVB', 'O3'],\n",
            "      dtype='object')\n",
            "(800, 12)\n",
            "(800, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO</th>\n",
              "      <th>PM10</th>\n",
              "      <th>PM25</th>\n",
              "      <th>NO</th>\n",
              "      <th>NOX</th>\n",
              "      <th>WD</th>\n",
              "      <th>RH</th>\n",
              "      <th>TEMP</th>\n",
              "      <th>WS</th>\n",
              "      <th>UVA</th>\n",
              "      <th>UVB</th>\n",
              "      <th>O3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>800.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.192715</td>\n",
              "      <td>0.125446</td>\n",
              "      <td>0.129129</td>\n",
              "      <td>0.086484</td>\n",
              "      <td>0.151351</td>\n",
              "      <td>0.691304</td>\n",
              "      <td>0.747036</td>\n",
              "      <td>0.547507</td>\n",
              "      <td>0.147604</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>0.031876</td>\n",
              "      <td>0.455314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.086517</td>\n",
              "      <td>0.050579</td>\n",
              "      <td>0.055469</td>\n",
              "      <td>0.067396</td>\n",
              "      <td>0.080386</td>\n",
              "      <td>0.085392</td>\n",
              "      <td>0.131786</td>\n",
              "      <td>0.124855</td>\n",
              "      <td>0.024504</td>\n",
              "      <td>0.005416</td>\n",
              "      <td>0.007052</td>\n",
              "      <td>0.130818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023560</td>\n",
              "      <td>0.026217</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.014138</td>\n",
              "      <td>0.569363</td>\n",
              "      <td>0.155555</td>\n",
              "      <td>0.028733</td>\n",
              "      <td>0.073525</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.095223</td>\n",
              "      <td>0.097378</td>\n",
              "      <td>0.049619</td>\n",
              "      <td>0.109911</td>\n",
              "      <td>0.642404</td>\n",
              "      <td>0.655275</td>\n",
              "      <td>0.482882</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.025956</td>\n",
              "      <td>0.028152</td>\n",
              "      <td>0.377246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.172413</td>\n",
              "      <td>0.117801</td>\n",
              "      <td>0.119850</td>\n",
              "      <td>0.078498</td>\n",
              "      <td>0.139840</td>\n",
              "      <td>0.660832</td>\n",
              "      <td>0.754444</td>\n",
              "      <td>0.569800</td>\n",
              "      <td>0.147704</td>\n",
              "      <td>0.029492</td>\n",
              "      <td>0.033750</td>\n",
              "      <td>0.455090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.241378</td>\n",
              "      <td>0.143979</td>\n",
              "      <td>0.149813</td>\n",
              "      <td>0.107443</td>\n",
              "      <td>0.172762</td>\n",
              "      <td>0.699512</td>\n",
              "      <td>0.851111</td>\n",
              "      <td>0.629997</td>\n",
              "      <td>0.159535</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.036701</td>\n",
              "      <td>0.532934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.655759</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.821360</td>\n",
              "      <td>0.522228</td>\n",
              "      <td>0.054769</td>\n",
              "      <td>0.043509</td>\n",
              "      <td>0.910180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               CO        PM10        PM25          NO         NOX          WD  \\\n",
              "count  800.000000  800.000000  800.000000  800.000000  800.000000  800.000000   \n",
              "mean     0.192715    0.125446    0.129129    0.086484    0.151351    0.691304   \n",
              "std      0.086517    0.050579    0.055469    0.067396    0.080386    0.085392   \n",
              "min      0.000000    0.023560    0.026217    0.001050    0.014138    0.569363   \n",
              "25%      0.137931    0.095223    0.097378    0.049619    0.109911    0.642404   \n",
              "50%      0.172413    0.117801    0.119850    0.078498    0.139840    0.660832   \n",
              "75%      0.241378    0.143979    0.149813    0.107443    0.172762    0.699512   \n",
              "max      0.655172    0.655759    1.000000    1.000000    1.000000    1.000000   \n",
              "\n",
              "               RH        TEMP          WS         UVA         UVB          O3  \n",
              "count  800.000000  800.000000  800.000000  800.000000  800.000000  800.000000  \n",
              "mean     0.747036    0.547507    0.147604    0.028320    0.031876    0.455314  \n",
              "std      0.131786    0.124855    0.024504    0.005416    0.007052    0.130818  \n",
              "min      0.155555    0.028733    0.073525    0.000000    0.000000    0.017964  \n",
              "25%      0.655275    0.482882    0.135793    0.025956    0.028152    0.377246  \n",
              "50%      0.754444    0.569800    0.147704    0.029492    0.033750    0.455090  \n",
              "75%      0.851111    0.629997    0.159535    0.031894    0.036701    0.532934  \n",
              "max      1.000000    0.821360    0.522228    0.054769    0.043509    0.910180  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvh2eyWXwKi9",
        "colab_type": "text"
      },
      "source": [
        "**b)** Ahora nos gustarı́a manipular los datos, para que hagamos la predicción para el tiempo siguiente usando los valores de los últimos perı́odos de tiempo. El número de perı́odos de tiempos \"hacia atrás\" que usaremos se denomina *lag*. Por ejemplo, tendremos un *lag* igual a 3, si para predecir el valor $x_{t+1}$ en el tiempo siguiente usamos la información del tiempo actual $x_t$ y la de los dos perı́odos anteriores $x_{t-1}$ y $x_{t-2}$ como variables de entrada. Realice una función que reciba una secuencia de valores y la transforme en dos arreglos *dataX* (inputs) y *dataY* (targets), además utilicela para generar los conjuntos de entrenamiento y test para el problema decidiendo el valor del *lag*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8M5iC-SwKi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_dataset(dataset,lag=1):\n",
        "    return np.array(dataX),np.array(dataY)\n",
        "\n",
        "# Por ejemplo si en el dataset tenemos el arreglo 20.7,17.9,18.8,14.6,15.8,15.8,10.1.\n",
        "\n",
        "create_dataset(dataset,3)\n",
        "\n",
        "'''\n",
        "La función debiese generar $(X_1,X_2,X_3)$ e $Y$:\n",
        "\n",
        "|$X_0$|$X_1$|$X_2$|Y|\n",
        "|---|---|---|---|\n",
        "|20.7|17.9|18.8|14.6|\n",
        "|17.9|18.8|14.6|15.8|\n",
        "|18.8|14.6|15.8|15.8|\n",
        "|14.6|15.8|15.8|10.1|\n",
        "'''\n",
        "\n",
        "# lag = #your call\n",
        "trainX, trainY = create_dataset(stream_train_scaled, lag)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR7gLgsqwKjD",
        "colab_type": "text"
      },
      "source": [
        "**c)** En estos momentos tenemos nuestros datos en la forma [*samples, features*]. Sin embargo, la red RNN necesita que los datos se encuentren en un arreglo de tres dimensiones [*samples, time steps, features*]. Transforme el  conjuntos de entrenamiento y de pruebas a la estructura deseada, incorporando el *lag* como *features*. Defina dos modelos para comparar, una red recurrente simple y una red convolucional unidimensional (donde el kernel es un vector de largo $w$), comente sobre las dimensiones/*shape* de los parámetros de cada una de las redes. Entrene los dos modelos con la nueva representación ¿Qué limitaciones tiene el tamaño/*width* del kernel de la convolucional?."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z46uFhEwKjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,SimpleRNN\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(SimpleRNN(units=4, input_shape=(1,lag), activation='tanh',return_sequences=False,go_backwards=False))\n",
        "model_rnn.add(Dense(1,activation='linear'))\n",
        "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model_rnn.summary()\n",
        "rnn_weights = model_rnn.get_weights()\n",
        "model_rnn.fit(trainX, trainY, epochs=25, batch_size=1, verbose=1)\n",
        "\n",
        "#and now CNN\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Conv1D(4, kernel_size=?,input_shape=(1,lag), activation='relu',padding='valid')) \n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(1,activation='linear'))\n",
        "model_cnn.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model_cnn.summary()\n",
        "cnn_weights = model_cnn.get_weights()\n",
        "model_cnn.fit(trainX, trainY, epochs=25, batch_size=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDvQll1ewKjL",
        "colab_type": "text"
      },
      "source": [
        "**d)** Mida desempeño de los dos modelos a través del *root mean square error* (RMSE) y compare. Para ésto deberá realizar las predicciones sobre el conjunto de entrenamiento y pruebas/validación, recuerde denormalizar los datos para que el error pueda ser computado en la escala original del rango de valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ix4KGrfwKjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainPredict = model.predict(trainX,batch_size=batch_size)\n",
        "trainPredict = scaler.inverse_transform(trainP)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRuPzh7pwKjQ",
        "colab_type": "text"
      },
      "source": [
        "**e)** Grafique las predicciones del conjunto de entrenamiento y pruebas/validación, y contrástelas con la serie de tiempo original. Muestre un extracto de la predicción para ver en mas detalle cómo es la predicción del modelo, comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMa-g7fwwKjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(dataframe.values)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(dataframe.values)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[(len(trainPredict)+2*lag):, :] = testPredic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfkDMTrWwKjV",
        "colab_type": "text"
      },
      "source": [
        "**f)** En lugar de aumentar el número de dimensiones como el el paso c), entrene la red con un *timestep=lag* y con dimensión de entrada = 1. Compare y comente sobre el cambio de dimensiones y número de parámetros de las redes ¿Cuál tipo de red tiene más parámetros y porqué? ¿Se produce una mejora del error de entrenamiento y pruebas? ¿Los tiempos de computación son comparables? Comente brevemente sobre cuál es la forma correcta para aprovechar la información a través del tiempo, si con esta forma o la realizada en el paso e)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgql-dmnwKjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(SimpleRNN(units=4, input_shape=(lag,1), activation='tanh',return_sequences=False,go_backwards=False))\n",
        "model_rnn.add(Dense(1,activation='linear'))\n",
        "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model_rnn.fit(trainX, trainY, epochs=25, batch_size=1, verbose=1)\n",
        "model_rnn.summary()\n",
        "\n",
        "#and now CNN\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Conv1D(4, kernel_size=3,input_shape=(lag,1), activation='relu',padding='valid')) # or kernel=2\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(1,activation='linear'))\n",
        "model_cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWEgcRepwKjf",
        "colab_type": "text"
      },
      "source": [
        "**g)** Compare el desempeño de ambos modelos al variar el *lag* definido en c), recuerde que puede variarlo entre 1 y $T_{max}$. Comente brevemente sobre qué resulta mejor, el tener más información para predecir o si ésto satura la predicción por el efecto del gradiente desvaneciente ¿Ésto ocurre sólo con el modelo recurrente o con el convolucional también? ¿Por qué?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_AyC99EwKjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxq1zCIYwKjn",
        "colab_type": "text"
      },
      "source": [
        "**h)** Como habrá notado, si usa una ventana de tiempo/*timesteps* muy larga ($T$ grande), las redes tienen dificultades para aprender. Para abordar este problema se le pide experimentar con la siguiente idea: dividir una secuencia de largo $T$ en trozos de largo $K$ y representarla como una matriz de $T/K$ columnas (asumamos que $T$ es un múltiplo de $K$) y $K$ filas, y entrenar la red para procesar la secuencia formada las columnas de esa matriz. De este modo, la red debe aprender dependencias temporales más cortas (largo $T/K$ en vez de $T$), pero accede a toda la información original en forma de atributos ($K$). Llamaremos a esta idea “**time folding**”. Se le recomienda usar un valor de $K$ que tenga sentido en el problema (por ejemplo $K=7$ representaría una semana). Defina y entrene los modelos para ésta nueva representación.  \n",
        "¿Cómo se ven afectados los modelos?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl1Vq8ZNwKjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_rnn = ?\n",
        "# model_cnn = ?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT7TTHYNwKjw",
        "colab_type": "text"
      },
      "source": [
        "**i)** [Opcional] Experimente con variar el número de unidades de la red recurrente (RNN) y el número de filtros de las capas convolucionales. Comente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUvE8h21wKjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzvLAcQqwKj2",
        "colab_type": "text"
      },
      "source": [
        "**j)** [Opcional] Experimente con variar el *width* de los filtros convolucionales ¿Cuál sería el efecto de ésto? ¿Qué limite tiene (mayor y menor)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlS_J1Y6wKj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAnar1XJwKj9",
        "colab_type": "text"
      },
      "source": [
        "#### Agregar otras mediciones como input:  $CO, PM10, NO, NO_x,...,UVB, O_3$\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://image.slidesharecdn.com/scaled-ml-2018-180529212354/95/role-of-tensors-in-machine-learning-2-638.jpg?cb=1527629345\" title=\"tensors\" width=\"65%\"/>\n",
        "\n",
        "*Decida si continuar con la convolucional 1D o cambiar a convolucional 2D, comente sobre ésta decisión.*\n",
        "\n",
        "**k)** Agregue las otras mediciones como nuevos valores de entrada al modelo definido en f), por lo que ahora se transforma la serie de tiempo a N-Dimensional, se tiene un *timestep=lag* con un *features*= mediciones nuevas + Ozono. Defina los nuevos modelos con el mejor *lag* encontrado en g)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hnQG0wHwKj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ejemplo:\n",
        "new_sequences = np.vstack([co_seq,pm10_seq,...,O3_seq]) # (Contaminantes, dias de verano entre 2003-2016)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0Mq1BvwKkG",
        "colab_type": "text"
      },
      "source": [
        "**l)** Entrene los modelos  ¿Se observan los mismos resultados? ¿Las mediciones incorporadas como información adicional ayudan al modelo? Comente y compare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YAYlvfT-Nqhw"
      },
      "source": [
        "<a id=\"refs\"></a>\n",
        "## Referencias\n",
        "[1] Borucki, W. J., Koch, D. G., Basri, G., Batalha, N., Boss, A., Brown, T. M., ... & Dunham, E. W. (2011). *Characteristics of Kepler planetary candidates based on the first data set*. The Astrophysical Journal, 728(2), 117. also in: https://exoplanetarchive.ipac.caltech.edu/index.html  \n",
        "[2] Bugueno, M., Mena, F., & Araya, M. *Refining Exoplanet Detection Using Supervised Learning and Feature Engineering*.  \n",
        "[3] Sutskever, I., Martens, J., Dahl, G. E., & Hinton, G. E. (2013). *On the importance of initialization and momentum in deep learning*. ICML (3), 28(1139-1147), 5.  \n",
        "[4] Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). *Focal loss for dense object detection*. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).  \n",
        "[5] Chen, P., Chen, Y., & Rao, M. (2008). *Metrics defined by Bregman divergences: Part 2*. Communications in Mathematical Sciences, 6(4), 927-948.  \n",
        "[6] Glorot, X., & Bengio, Y. (2010, March). *Understanding the difficulty of training deep feedforward neural networks*. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).   \n",
        "[7] He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving deep into rectifiers: Surpassing human-level performance on imagenet classification*. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).    \n",
        "[8] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). *Dropout: a simple way to prevent neural networks from overfitting*. The Journal of Machine Learning Research, 15(1), 1929-1958.  \n",
        "[9] Ioffe, S., & Szegedy, C. (2015). Batch normalization: *Accelerating deep network training by reducing internal covariate shift*. arXiv preprint arXiv:1502.03167.  \n",
        "[10] Krizhevsky, A., Nair, V., & Hinton, G. (2014). *The CIFAR-10 dataset*. online: http://www.cs.toronto.edu/kriz/cifar.html , 4.  \n",
        "[11] Bengio, Y. (2012, June). *Deep learning of representations for unsupervised and transfer learning*. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning (pp. 17-36).  \n",
        "[12] Simonyan, K., & Zisserman, A. (2014). *Very deep convolutional networks for large-scale image recognition*. arXiv preprint arXiv:1409.1556.  \n",
        "[13] Ruder, S. (2017). *An overview of multi-task learning in deep neural networks*. arXiv preprint arXiv:1706.05098.  "
      ]
    }
  ]
}